import os
import json
import subprocess
import time
import pandas as pd
from beijing_data_download_config import USER_KEY, DATASETS

OUTPUT_DIR = "../../assets/datasets/beijing_data"
os.makedirs(OUTPUT_DIR, exist_ok=True)


def fetch_page_with_curl(url):
    """ä½¿ç”¨ curl è·å–å•é¡µæ•°æ®ï¼Œè‡ªåŠ¨é‡è¯•"""
    for attempt in range(3):
        try:
            result = subprocess.run(
                ["curl", "-s", "--connect-timeout", "15", url],
                capture_output=True,
                text=True,
                encoding="utf-8",
                check=True
            )
            return json.loads(result.stdout)
        except (subprocess.CalledProcessError, json.JSONDecodeError) as e:
            print(f"  âš ï¸ é‡è¯• {attempt + 1}/3: {e}")
            time.sleep(2)
    raise RuntimeError(f"æ— æ³•è·å– URL: {url}")


def download_dataset(name, content_id, page_size=100):
    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½æ•°æ®é›†: {name} (contentId: {content_id})")

    all_records = []
    page = 1
    total_count = None

    while True:
        url = f"https://data.beijing.gov.cn/cms/web/api/{USER_KEY}/{content_id}?currentPage={page}&pageSize={page_size}"
        print(f"  â†’ è¯·æ±‚ç¬¬ {page} é¡µ...")

        try:
            data = fetch_page_with_curl(url)
        except Exception as e:
            print(f"  âŒ è·å–å¤±è´¥: {e}")
            break

        if data.get("status") != 200:
            print(f"  âŒ API é”™è¯¯: {data.get('msg')}")
            break

        records = data.get("object", [])
        if not records:
            print("  ğŸ“­ æ— æ›´å¤šæ•°æ®ï¼Œç»“æŸã€‚")
            break

        all_records.extend(records)
        print(f"    + è·å– {len(records)} æ¡ï¼Œç´¯è®¡ {len(all_records)} æ¡")

        # é¦–æ¬¡è¯·æ±‚æ—¶è·å–æ€»æ•°ï¼ˆç”¨äºè¿›åº¦é¢„ä¼°ï¼‰
        if total_count is None:
            page_info = data.get("page", {})
            total_count = page_info.get("count", len(records))
            total_pages = page_info.get("countPage", 1)
            print(f"    ğŸ“Š æ€»è®°å½•æ•°: {total_count}, é¢„è®¡æ€»é¡µæ•°: {total_pages}")

        # æå‰ç»ˆæ­¢ï¼ˆå¦‚æœå·²è·å–å…¨éƒ¨ï¼‰
        if len(all_records) >= total_count:
            break

        page += 1
        # time.sleep(0.5)  # ç¤¼è²Œæ€§å»¶è¿Ÿ

    # ä¿å­˜ä¸ºå¤šç§æ ¼å¼
    base_path = os.path.join(OUTPUT_DIR, name.replace("/", "_").replace("\\", "_"))

    # JSON
    with open(f"{base_path}.json", "w", encoding="utf-8") as f:
        json.dump({"meta": {"dataset": name, "total": len(all_records)}, "beijing_data": all_records}, f, ensure_ascii=False,
                  indent=2)

    # CSV
    if all_records:
        df = pd.DataFrame(all_records)
        df.to_csv(f"{base_path}.csv", index=False, encoding="utf-8-sig")  # utf-8-sig æ”¯æŒ Excel ä¸­æ–‡

    print(f"âœ… ä¸‹è½½å®Œæˆï¼å…± {len(all_records)} æ¡ï¼Œå·²ä¿å­˜è‡³ {base_path}.[json/csv]")


def main():
    for dataset in DATASETS:
        try:
            download_dataset(
                name=dataset["name"],
                content_id=dataset["content_id"],
                page_size=dataset.get("page_size", 100)
            )
        except Exception as e:
            print(f"ğŸ’¥ æ•°æ®é›† {dataset['name']} ä¸‹è½½å¤±è´¥: {e}")
            continue


if __name__ == "__main__":
    main()
